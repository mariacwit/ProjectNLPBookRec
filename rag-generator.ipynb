{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NEW DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:49:34.419094Z",
     "iopub.status.busy": "2024-12-14T07:49:34.418703Z",
     "iopub.status.idle": "2024-12-14T07:51:47.451426Z",
     "shell.execute_reply": "2024-12-14T07:51:47.450550Z",
     "shell.execute_reply.started": "2024-12-14T07:49:34.419053Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolina/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "100%|██████████| 676/676 [03:09<00:00,  3.57it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  title  \\\n",
      "78                                La danza de la muerte   \n",
      "1298  At the Mountains of Madness and Other Novels o...   \n",
      "2095                                   The Untouchables   \n",
      "2396   Aeneid: Selections from Books 1  2  4  6  10  12   \n",
      "3844                  Diablo II Ultimate Strategy Guide   \n",
      "\n",
      "                                              authors  average_rating  \\\n",
      "78                    Stephen King/Eduardo Goligorsky            4.34   \n",
      "1298  H.P. Lovecraft/August Derleth/E. Hoffmann Price            4.44   \n",
      "2095                          Eliot Ness/Oscar Fraley            3.89   \n",
      "2396                       Virgil/Barbara Weiden Boyd            4.35   \n",
      "3844                                   Bart G. Farkas            3.81   \n",
      "\n",
      "                                                  genre  \\\n",
      "78    Based on the title \"La danza de la muerte,\" a ...   \n",
      "1298  The genre for the book \"At the Mountains of Ma...   \n",
      "2095  Based on the title \"The Untouchables,\" the gen...   \n",
      "2396  The genre for the book \"Aeneid: Selections fro...   \n",
      "3844  The genre for the book \"Diablo II Ultimate Str...   \n",
      "\n",
      "                   description  \n",
      "78    No description available  \n",
      "1298  No description available  \n",
      "2095  No description available  \n",
      "2396  No description available  \n",
      "3844  No description available  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load the new dataset\n",
    "df_books = pd.read_csv('FullDataSetGenreAndDescription.csv')\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_books = df_books[['title', 'authors', 'description', 'average_rating', 'num_pages', 'ratings_count', 'isbn13', 'genre']]\n",
    "\n",
    "# Drop rows with missing descriptions\n",
    "df_books.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "# Reset index after cleaning\n",
    "df_books.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Load pre-trained DPR Context Encoder and Tokenizer\n",
    "model_name = 'facebook/dpr-ctx_encoder-single-nq-base'\n",
    "context_encoder = DPRContextEncoder.from_pretrained(model_name)\n",
    "tokenizer = DPRContextEncoderTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "context_encoder.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encode_descriptions(descriptions, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(descriptions), batch_size)):\n",
    "        batch = descriptions[i:i + batch_size].tolist()\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = context_encoder(**inputs).pooler_output\n",
    "        embeddings.append(outputs.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Encode descriptions\n",
    "embeddings = encode_descriptions(df_books[\"description\"])\n",
    "\n",
    "# Initialize a FAISS index (L2 similarity)\n",
    "dimension = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index for later use\n",
    "faiss.write_index(faiss_index, \"book_retrieval_index.faiss\")\n",
    "\n",
    "# Function to parse the user query for author, description preferences, and genre\n",
    "def parse_user_query(query):\n",
    "    author = None\n",
    "    genre = None\n",
    "    \n",
    "    # Extract author if mentioned\n",
    "    if 'by' in query:\n",
    "        author_match = re.search(r'by ([\\w\\s]+)', query)\n",
    "        if author_match:\n",
    "            author = author_match.group(1).strip()\n",
    "\n",
    "    # Extract genre if mentioned\n",
    "    if 'genre' in query:\n",
    "        genre_match = re.search(r'genre ([\\w\\s]+)', query)\n",
    "        if genre_match:\n",
    "            genre = genre_match.group(1).strip()\n",
    "\n",
    "    # Remaining query as description keywords\n",
    "    description_keywords = query.split()\n",
    "\n",
    "    return author, genre, description_keywords\n",
    "\n",
    "# Function to retrieve books based on user query\n",
    "def retrieve_books(query, top_n=5):\n",
    "    author, genre, description_keywords = parse_user_query(query)\n",
    "    \n",
    "    # Formulate a query based on parsed description keywords\n",
    "    description_query = \" \".join(description_keywords)\n",
    "    \n",
    "    # Encode the description query\n",
    "    query_embedding = context_encoder(**tokenizer([description_query], return_tensors='pt', padding=True, truncation=True).to(device)).pooler_output\n",
    "    query_embedding = query_embedding.cpu().detach().numpy()\n",
    "    \n",
    "    # Search the FAISS index for the top_n most similar books\n",
    "    distances, indices = faiss_index.search(query_embedding, top_n)\n",
    "    \n",
    "    # Retrieve the details of the top books\n",
    "    results = df_books.iloc[indices[0]]\n",
    "\n",
    "    # Filter results by author if mentioned\n",
    "    if author:\n",
    "        results = results[results['authors'].str.contains(author, case=False, na=False)]\n",
    "\n",
    "    # Filter results by genre if mentioned\n",
    "    if genre:\n",
    "        results = results[results['genre'].str.contains(genre, case=False, na=False)]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "sample_query = \"Give me a thiller book\"\n",
    "top_books = retrieve_books(sample_query, top_n=5)\n",
    "print(top_books[['title', 'authors', 'average_rating', 'genre', 'description']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:58:18.804348Z",
     "iopub.status.busy": "2024-12-14T07:58:18.804029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.10.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:58:27.885129Z",
     "iopub.status.busy": "2024-12-14T07:58:27.884834Z",
     "iopub.status.idle": "2024-12-14T08:00:23.043831Z",
     "shell.execute_reply": "2024-12-14T08:00:23.042871Z",
     "shell.execute_reply.started": "2024-12-14T07:58:27.885101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=\"XXX\")\n",
    "\n",
    "# Load the dataset\n",
    "df_books = pd.read_csv(\"FullDataSetGenreAndDescription.csv\")\n",
    "\n",
    "# Drop rows with missing descriptions\n",
    "df_books.dropna(subset=['description'], inplace=True)\n",
    "df_books.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Load the pre-trained DPR Context Encoder and Tokenizer\n",
    "model_name = 'facebook/dpr-ctx_encoder-single-nq-base'\n",
    "context_encoder = DPRContextEncoder.from_pretrained(model_name)\n",
    "tokenizer = DPRContextEncoderTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "context_encoder.to(device)\n",
    "\n",
    "# Function to encode descriptions using DPR\n",
    "def encode_descriptions(descriptions, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(descriptions), batch_size):\n",
    "        batch = descriptions[i:i + batch_size].tolist()\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = context_encoder(**inputs).pooler_output\n",
    "        embeddings.append(outputs.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Encode the book descriptions and create FAISS index\n",
    "embeddings = encode_descriptions(df_books[\"description\"])\n",
    "faiss_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "# Function to parse the user query for preferences like author, genre, and rating\n",
    "def parse_user_query(query):\n",
    "    preferences = {\n",
    "        \"genre\": None,\n",
    "        \"author\": None,\n",
    "        \"rating_min\": None,\n",
    "        \"rating_max\": None,\n",
    "        \"keywords\": []\n",
    "    }\n",
    "    \n",
    "    # Extract genre, author, rating, and keywords from the query\n",
    "    genres = [\"romance\", \"mystery\", \"thriller\", \"fantasy\", \"science fiction\", \"non-fiction\", \"historical\", \"adventure\"]\n",
    "    for genre in genres:\n",
    "        if genre.lower() in query.lower():\n",
    "            preferences[\"genre\"] = genre\n",
    "    \n",
    "    author_pattern = r\"by\\s([A-Za-z\\s]+)\"\n",
    "    author_match = re.search(author_pattern, query)\n",
    "    if author_match:\n",
    "        preferences[\"author\"] = author_match.group(1).strip()\n",
    "\n",
    "    rating_min_pattern = r\"(above|at least|greater than)\\s(\\d(\\.\\d+)?)\"\n",
    "    rating_max_pattern = r\"(below|under)\\s(\\d(\\.\\d+)?)\"\n",
    "    \n",
    "    min_rating_match = re.search(rating_min_pattern, query)\n",
    "    max_rating_match = re.search(rating_max_pattern, query)\n",
    "    \n",
    "    if min_rating_match:\n",
    "        preferences[\"rating_min\"] = float(min_rating_match.group(2))\n",
    "    if max_rating_match:\n",
    "        preferences[\"rating_max\"] = float(max_rating_match.group(2))\n",
    "\n",
    "    keyword_list = [\"happy ending\", \"fast-paced\", \"slow burn\", \"adventure\", \"unexpected twists\", \"magical realism\"]\n",
    "    for keyword in keyword_list:\n",
    "        if keyword.lower() in query.lower():\n",
    "            preferences[\"keywords\"].append(keyword)\n",
    "    \n",
    "    return preferences\n",
    "\n",
    "# Function to retrieve top N books using DPR\n",
    "def retrieve_books(query, top_n=5):\n",
    "    query_embedding = context_encoder(**tokenizer([query], return_tensors='pt', padding=True, truncation=True).to(device)).pooler_output\n",
    "    query_embedding = query_embedding.cpu().detach().numpy()\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_n)\n",
    "    \n",
    "    # Retrieve book details\n",
    "    results = df_books.iloc[indices[0]]\n",
    "    return results\n",
    "\n",
    "# Function to format the prompt for GPT to generate a recommendation\n",
    "def format_prompt(user_query, filtered_books):\n",
    "    prompt = f\"User query: \\\"{user_query}\\\"\\n\\nBased on the following book descriptions, provide a personalized book recommendation:\\n\"\n",
    "    for i, book in enumerate(filtered_books, start=1):\n",
    "        prompt += f\"\\nBook {i}: \\nTitle: {book['title']}\\nAuthor(s): {book['authors']}\\nRating: {book['average_rating']}\\nDescription: {book['description']}\\n\"\n",
    "    prompt += \"\\nProvide your recommendation and explain why it suits the user's preferences.\"\n",
    "    return prompt\n",
    "\n",
    "# Function to get GPT-3.5 response\n",
    "def get_gpt_response(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in book recommendations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Sample user query looking for a \"thriller\" book\n",
    "#sample_user_query = \"Can you recommend a thriller book?\"\n",
    "\n",
    "# Parse the user query to extract preferences\n",
    "#preferences = parse_user_query(sample_user_query)\n",
    "\n",
    "# Retrieve top 5 books based on preferences\n",
    "#top_books = retrieve_books(sample_user_query, top_n=5)\n",
    "\n",
    "# Print the retrieved top books\n",
    "#print(\"Top 5 Books Suggested:\")\n",
    "#print(top_books[['title', 'authors', 'average_rating', 'description']])\n",
    "\n",
    "# Format the prompt for GPT to generate a personalized recommendation\n",
    "#prompt = format_prompt(sample_user_query, top_books.to_dict(orient='records'))\n",
    "\n",
    "# Get the recommendation from GPT\n",
    "#recommendation = get_gpt_response(prompt)\n",
    "\n",
    "# Print the recommendation\n",
    "#print(\"\\nGenerated Recommendation:\")\n",
    "#print(recommendation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Books Suggested:\n",
      "                                                   title  \\\n",
      "548                           The Private Parts of Women   \n",
      "5965                       The Complete Dream Dictionary   \n",
      "2888   Waiting for the Party: The Life of Frances Hod...   \n",
      "9013                           Fire Ice (NUMA Files  #3)   \n",
      "10460                                             Tinsel   \n",
      "\n",
      "                            authors  average_rating  \\\n",
      "548                 Lesley Glaister            3.80   \n",
      "5965                    Pamela Ball            3.68   \n",
      "2888                    Ann Thwaite            3.80   \n",
      "9013   Clive Cussler/Paul Kemprecos            3.92   \n",
      "10460               William Goldman            3.19   \n",
      "\n",
      "                                             description  \n",
      "548    A handsome new cover edition of Lesley Glaiste...  \n",
      "5965           A practical guide to interpreting dreams.  \n",
      "2888         A biography of the author of Secret Garden.  \n",
      "9013                            A Kurt Austin adventure.  \n",
      "10460          A novel of Hollywood's best-kept secrets.  \n",
      "\n",
      "Generated Recommendation:\n",
      "Based on the book descriptions provided, I recommend \"Fire Ice\" by Clive Cussler and Paul Kemprecos. This thriller novel is part of the NUMA Files series and follows the exciting adventures of Kurt Austin. If you enjoy compelling and disturbing narratives with elements of mystery and suspense, \"Fire Ice\" is a great choice for you. The action-packed storyline and engaging characters are sure to keep you on the edge of your seat throughout the book. Give it a try for a thrilling reading experience!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample user query looking for a \"thriller\" book\n",
    "sample_user_query = \"Can you recommend a thriller book?\"\n",
    "\n",
    "# Parse the user query to extract preferences\n",
    "preferences = parse_user_query(sample_user_query)\n",
    "\n",
    "# Retrieve top 5 books based on preferences\n",
    "top_books = retrieve_books(sample_user_query, top_n=5)\n",
    "\n",
    "# Print the retrieved top books\n",
    "print(\"Top 5 Books Suggested:\")\n",
    "print(top_books[['title', 'authors', 'average_rating', 'description']])\n",
    "\n",
    "# Format the prompt for GPT to generate a personalized recommendation\n",
    "prompt = format_prompt(sample_user_query, top_books.to_dict(orient='records'))\n",
    "\n",
    "# Get the recommendation from GPT\n",
    "recommendation = get_gpt_response(prompt)\n",
    "\n",
    "# Print the recommendation\n",
    "print(\"\\nGenerated Recommendation:\")\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T08:00:23.045469Z",
     "iopub.status.busy": "2024-12-14T08:00:23.045206Z",
     "iopub.status.idle": "2024-12-14T08:02:36.102302Z",
     "shell.execute_reply": "2024-12-14T08:02:36.101487Z",
     "shell.execute_reply.started": "2024-12-14T08:00:23.045444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "100%|██████████| 676/676 [03:01<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset (make sure it's uploaded to Kaggle environment)\n",
    "df_books = pd.read_csv('FullDataSetGenreAndDescription.csv')\n",
    "\n",
    "# Keep only relevant columns and drop rows with missing descriptions\n",
    "df_books = df_books[['title', 'authors', 'description', 'average_rating', 'num_pages', 'ratings_count', 'isbn13', 'genre']]\n",
    "df_books.dropna(subset=['description'], inplace=True)\n",
    "df_books.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Load pre-trained DPR Context Encoder and Tokenizer\n",
    "model_name = 'facebook/dpr-ctx_encoder-single-nq-base'\n",
    "context_encoder = DPRContextEncoder.from_pretrained(model_name)\n",
    "tokenizer = DPRContextEncoderTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "context_encoder.to(device)\n",
    "\n",
    "# Function to encode book descriptions using DPR\n",
    "def encode_descriptions(descriptions, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(descriptions), batch_size)):\n",
    "        batch = descriptions[i:i + batch_size].tolist()\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = context_encoder(**inputs).pooler_output\n",
    "        embeddings.append(outputs.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Encode descriptions and create FAISS index\n",
    "embeddings = encode_descriptions(df_books[\"description\"])\n",
    "dimension = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "# Function to parse the user query (for personalized quiz answers)\n",
    "def parse_user_query(query):\n",
    "    author = None\n",
    "    genre = None\n",
    "    \n",
    "    # Extract author if mentioned\n",
    "    if 'by' in query:\n",
    "        author_match = re.search(r'by ([\\w\\s]+)', query)\n",
    "        if author_match:\n",
    "            author = author_match.group(1).strip()\n",
    "\n",
    "    # Extract genre if mentioned\n",
    "    if 'genre' in query:\n",
    "        genre_match = re.search(r'genre ([\\w\\s]+)', query)\n",
    "        if genre_match:\n",
    "            genre = genre_match.group(1).strip()\n",
    "\n",
    "    # Remaining query as description keywords\n",
    "    description_keywords = query.split()\n",
    "\n",
    "    return author, genre, description_keywords\n",
    "\n",
    "# Function to retrieve books based on the query\n",
    "def retrieve_books(query, top_n=5):\n",
    "    author, genre, description_keywords = parse_user_query(query)\n",
    "    description_query = \" \".join(description_keywords)\n",
    "    \n",
    "    # Encode the description query\n",
    "    query_embedding = context_encoder(**tokenizer([description_query], return_tensors='pt', padding=True, truncation=True).to(device)).pooler_output\n",
    "    query_embedding = query_embedding.cpu().detach().numpy()\n",
    "    \n",
    "    # Search the FAISS index for the top_n most similar books\n",
    "    distances, indices = faiss_index.search(query_embedding, top_n)\n",
    "    \n",
    "    # Retrieve the details of the top books\n",
    "    results = df_books.iloc[indices[0]]\n",
    "\n",
    "    # Filter by author if mentioned\n",
    "    if author:\n",
    "        results = results[results['authors'].str.contains(author, case=False, na=False)]\n",
    "\n",
    "    # Filter by genre if mentioned\n",
    "    if genre:\n",
    "        results = results[results['genre'].str.contains(genre, case=False, na=False)]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Quiz function to collect user preferences\n",
    "def personality_quiz():\n",
    "    print(\"Welcome to the Book Personality Quiz! Let's find the perfect book for you.\\n\")\n",
    "    \n",
    "    # Question 1: Genre Preference\n",
    "    genre = input(\"What genre do you prefer? (e.g., Fantasy, Mystery, Romance, Thriller, Sci-Fi): \")\n",
    "    \n",
    "    # Question 2: Book Length Preference\n",
    "    book_length = input(\"Do you prefer shorter books (under 300 pages) or longer ones? (Short/Long): \")\n",
    "    \n",
    "    # Question 3: Reading Pace\n",
    "    reading_pace = input(\"Do you prefer fast-paced books or slow, reflective ones? (Fast-paced/Slow-paced): \")\n",
    "    \n",
    "    # Formulate query based on user inputs\n",
    "    query = genre + \" \" + book_length + \" \" + reading_pace\n",
    "    \n",
    "    # Retrieve books based on the query\n",
    "    print(\"\\nBased on your preferences, here are some book recommendations:\\n\")\n",
    "    recommended_books = retrieve_books(query, top_n=5)\n",
    "    \n",
    "    # Display recommended books\n",
    "    if not recommended_books.empty:\n",
    "        for i, row in recommended_books.iterrows():\n",
    "            print(f\"Title: {row['title']}\")\n",
    "            print(f\"Author: {row['authors']}\")\n",
    "            print(f\"Average Rating: {row['average_rating']}\")\n",
    "            print(f\"Number of Pages: {row['num_pages']}\")\n",
    "            print(f\"Description: {row['description'][:200]}...\")  # Displaying first 200 characters of the description\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"Sorry, no recommendations found based on your preferences.\")\n",
    "\n",
    "# Run the personality quiz\n",
    "#personality_quiz()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T08:02:54.605299Z",
     "iopub.status.busy": "2024-12-14T08:02:54.604660Z",
     "iopub.status.idle": "2024-12-14T08:02:55.296394Z",
     "shell.execute_reply": "2024-12-14T08:02:55.295139Z",
     "shell.execute_reply.started": "2024-12-14T08:02:54.605265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def personality_quiz():\n",
    "    def submit_quiz():\n",
    "        genre = genre_var.get()\n",
    "        length = length_var.get()\n",
    "        pace = pace_var.get()\n",
    "\n",
    "        # Create a query based on user input\n",
    "        query = f\"{genre} {length} {pace}\"\n",
    "        \n",
    "        # Retrieve books based on the query\n",
    "        recommended_books = retrieve_books(query, top_n=5)\n",
    "        \n",
    "        # Format and display the recommended books\n",
    "        result = \"\\n\".join([f\"Title: {book['title']}\\nAuthor: {book['authors']}\\nRating: {book['average_rating']}\\nPages: {book['num_pages']}\\nDescription: {book['description'][:200]}...\\n{'-'*50}\" for book in recommended_books.to_dict(orient='records')])\n",
    "        \n",
    "        if not result:\n",
    "            result = \"Sorry, no recommendations found based on your preferences.\"\n",
    "\n",
    "        messagebox.showinfo(\"Quiz Recommendations\", f\"Based on your preferences, here are some books:\\n{result}\")\n",
    "\n",
    "    # Create a new window for the quiz\n",
    "    quiz_window = tk.Toplevel(root)\n",
    "    quiz_window.title(\"Book Personality Quiz\")\n",
    "\n",
    "    tk.Label(quiz_window, text=\"Welcome to the Book Personality Quiz! Let's find the perfect book for you.\", font=(\"Arial\", 50)).pack(pady=20)\n",
    "\n",
    "    # Genre question\n",
    "    tk.Label(quiz_window, text=\"What genre do you prefer? (e.g., Fantasy, Mystery, Romance, Thriller, Sci-Fi)\").pack()\n",
    "    genre_var = tk.StringVar()\n",
    "    tk.Entry(quiz_window, textvariable=genre_var).pack(pady=50)\n",
    "\n",
    "    # Length question\n",
    "    tk.Label(quiz_window, text=\"Do you prefer shorter books (under 300 pages) or longer ones? (Short/Long)\").pack()\n",
    "    length_var = tk.StringVar()\n",
    "    tk.Entry(quiz_window, textvariable=length_var).pack(pady=50)\n",
    "\n",
    "    # Pace question\n",
    "    tk.Label(quiz_window, text=\"Do you prefer fast-paced books or slow, reflective ones? (Fast-paced/Slow-paced)\").pack()\n",
    "    pace_var = tk.StringVar()\n",
    "    tk.Entry(quiz_window, textvariable=pace_var).pack(pady=50)\n",
    "\n",
    "    # Submit button to process the answers\n",
    "    tk.Button(quiz_window, text=\"Submit\", command=submit_quiz).pack(pady=20)\n",
    "\n",
    "\n",
    "def send_query():\n",
    "    user_query = str(user_input.get())\n",
    "\n",
    "    if not user_query.strip():\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a query before sending.\")\n",
    "        return\n",
    "\n",
    "    # Display user query in chat log\n",
    "    chat_log.config(state=tk.NORMAL)  # Allow editing of chat_log\n",
    "    chat_log.insert(tk.END, f\"You: {user_query}\\n\", \"user\")\n",
    "    chat_log.yview(tk.END)  # Scroll to the bottom\n",
    "    chat_log.config(state=tk.DISABLED)  # Disable editing after inserting\n",
    "\n",
    "    # Process user query and get response\n",
    "    preferences = parse_user_query(user_query)\n",
    "    books = retrieve_books(user_query, top_n=5)\n",
    "    prompt = format_prompt(user_query, books.to_dict(orient='records'))\n",
    "    response = get_gpt_response(prompt)\n",
    "\n",
    "    # Display bot's response in chat log\n",
    "    chat_log.config(state=tk.NORMAL)  # Allow editing of chat_log\n",
    "    chat_log.insert(tk.END, f\"Bot: {response}\\n\\n\", \"bot\")\n",
    "    chat_log.yview(tk.END)  # Scroll to the bottom\n",
    "    chat_log.config(state=tk.DISABLED)  # Disable editing after inserting\n",
    "\n",
    "    # Clear the user input field\n",
    "    user_input.delete(0, tk.END)\n",
    "\n",
    "\n",
    "# Main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Book Recommendation Chatbot\")\n",
    "root.geometry(\"1000x900\")  # Increased window size\n",
    "\n",
    "# Welcome message\n",
    "welcome_label = tk.Label(root, text=\"Thank you for using our Book Recommendation Chatbot!\", font=(\"Arial\", 700), pady=20)\n",
    "welcome_label.pack()\n",
    "\n",
    "# Chat log\n",
    "chat_frame = tk.Frame(root)\n",
    "chat_frame.pack(pady=20, fill=tk.BOTH, expand=True)\n",
    "\n",
    "chat_log = tk.Text(chat_frame, wrap=tk.WORD, state=tk.DISABLED, font=(\"Arial\", 60))  # Larger font size\n",
    "chat_log.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "scrollbar = tk.Scrollbar(chat_frame, command=chat_log.yview)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "chat_log[\"yscrollcommand\"] = scrollbar.set\n",
    "\n",
    "# Apply tags for formatting user and bot messages\n",
    "chat_log.tag_configure(\"user\", foreground=\"blue\", font=(\"Arial\", 60, \"bold\"))\n",
    "chat_log.tag_configure(\"bot\", foreground=\"green\", font=(\"Arial\", 60))\n",
    "\n",
    "# User input\n",
    "input_frame = tk.Frame(root)\n",
    "input_frame.pack(pady=40, fill=tk.X)\n",
    "\n",
    "user_input = tk.Entry(input_frame, font=(\"Arial\", 60))  # Larger font size\n",
    "user_input.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=10, pady=20)\n",
    "\n",
    "send_button = tk.Button(input_frame, text=\"Send\", command=send_query, font=(\"Arial\", 60))  # Larger button font\n",
    "send_button.pack(side=tk.RIGHT, padx=10)\n",
    "\n",
    "# Quiz button\n",
    "quiz_button = tk.Button(root, text=\"Take Our Quiz\", command=personality_quiz, font=(\"Arial\", 60), pady=30)  # Larger font for button\n",
    "quiz_button.pack(pady=30)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def personality_quiz():\n",
    "    def submit_quiz():\n",
    "        genre = genre_var.get()\n",
    "        length = length_var.get()\n",
    "        pace = pace_var.get()\n",
    "\n",
    "        # Create a query based on user input\n",
    "        query = f\"{genre} {length} {pace}\"\n",
    "        \n",
    "        # Retrieve books based on the query\n",
    "        recommended_books = retrieve_books(query, top_n=5)\n",
    "        \n",
    "        # Format and display the recommended books\n",
    "        result = \"\\n\".join([f\"Title: {book['title']}\\nAuthor: {book['authors']}\\nRating: {book['average_rating']}\\nPages: {book['num_pages']}\\nDescription: {book['description'][:200]}...\\n{'-'*50}\" for book in recommended_books.to_dict(orient='records')])\n",
    "        \n",
    "        if not result:\n",
    "            result = \"Sorry, no recommendations found based on your preferences.\"\n",
    "\n",
    "        messagebox.showinfo(\"Quiz Recommendations\", f\"Based on your preferences, here are some books:\\n{result}\")\n",
    "\n",
    "    # Create a new window for the quiz\n",
    "    quiz_window = tk.Toplevel(root)\n",
    "    quiz_window.title(\"Book Personality Quiz\")\n",
    "\n",
    "    tk.Label(quiz_window, text=\"Welcome to the Book Personality Quiz! Let's find the perfect book for you.\", font=(\"Arial\", 40)).pack(pady=20)\n",
    "\n",
    "    # Genre question\n",
    "    tk.Label(quiz_window, text=\"What genre do you prefer? (e.g., Fantasy, Mystery, Romance, Thriller, Sci-Fi)\", font=(\"Arial\", 30)).pack()\n",
    "    genre_var = tk.StringVar()\n",
    "    tk.Entry(quiz_window, textvariable=genre_var, font=(\"Arial\", 30)).pack(pady=10)\n",
    "\n",
    "    # Length question\n",
    "    tk.Label(quiz_window, text=\"Do you prefer shorter books (under 300 pages) or longer ones? (Short/Long)\", font=(\"Arial\", 30)).pack()\n",
    "    length_var = tk.StringVar()\n",
    "    tk.Entry(quiz_window, textvariable=length_var, font=(\"Arial\", 30)).pack(pady=10)\n",
    "\n",
    "    # Pace question\n",
    "    tk.Label(quiz_window, text=\"Do you prefer fast-paced books or slow, reflective ones? (Fast-paced/Slow-paced)\", font=(\"Arial\", 30)).pack()\n",
    "    pace_var = tk.StringVar()\n",
    "    tk.Entry(quiz_window, textvariable=pace_var, font=(\"Arial\", 30)).pack(pady=10)\n",
    "\n",
    "    # Submit button to process the answers\n",
    "    tk.Button(quiz_window, text=\"Submit\", command=submit_quiz, font=(\"Arial\", 30)).pack(pady=20)\n",
    "\n",
    "\n",
    "def send_query():\n",
    "    user_query = str(user_input.get())\n",
    "\n",
    "    if not user_query.strip():\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a query before sending.\")\n",
    "        return\n",
    "\n",
    "    # Display user query in chat log\n",
    "    chat_log.config(state=tk.NORMAL)  # Allow editing of chat_log\n",
    "    chat_log.insert(tk.END, f\"You: {user_query}\\n\", \"user\")\n",
    "    chat_log.yview(tk.END)  # Scroll to the bottom\n",
    "    chat_log.config(state=tk.DISABLED)  # Disable editing after inserting\n",
    "\n",
    "    # Process user query and get response\n",
    "    preferences = parse_user_query(user_query)\n",
    "    books = retrieve_books(user_query, top_n=5)\n",
    "    prompt = format_prompt(user_query, books.to_dict(orient='records'))\n",
    "    response = get_gpt_response(prompt)\n",
    "\n",
    "    # Display bot's response in chat log\n",
    "    chat_log.config(state=tk.NORMAL)  # Allow editing of chat_log\n",
    "    chat_log.insert(tk.END, f\"Bot: {response}\\n\\n\", \"bot\")\n",
    "    chat_log.yview(tk.END)  # Scroll to the bottom\n",
    "    chat_log.config(state=tk.DISABLED)  # Disable editing after inserting\n",
    "\n",
    "    # Clear the user input field\n",
    "    user_input.delete(0, tk.END)\n",
    "\n",
    "\n",
    "# Main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Book Recommendation Chatbot\")\n",
    "root.geometry(\"1400x1200\")  # Increased window size\n",
    "\n",
    "# Welcome message\n",
    "welcome_label = tk.Label(root, text=\"Thank you for using our Book Recommendation Chatbot!\", font=(\"Arial\", 36), pady=20)\n",
    "welcome_label.pack()\n",
    "\n",
    "# Chat log\n",
    "chat_frame = tk.Frame(root)\n",
    "chat_frame.pack(pady=20, fill=tk.BOTH, expand=True)\n",
    "\n",
    "chat_log = tk.Text(chat_frame, wrap=tk.WORD, state=tk.DISABLED, font=(\"Arial\", 28))  # Larger font size\n",
    "chat_log.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "scrollbar = tk.Scrollbar(chat_frame, command=chat_log.yview)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "chat_log[\"yscrollcommand\"] = scrollbar.set\n",
    "\n",
    "# Apply tags for formatting user and bot messages\n",
    "chat_log.tag_configure(\"user\", foreground=\"blue\", font=(\"Arial\", 28, \"bold\"))\n",
    "chat_log.tag_configure(\"bot\", foreground=\"green\", font=(\"Arial\", 28))\n",
    "\n",
    "# User input\n",
    "input_frame = tk.Frame(root)\n",
    "input_frame.pack(pady=20, fill=tk.X)\n",
    "\n",
    "user_input = tk.Entry(input_frame, font=(\"Arial\", 28))  # Larger font size\n",
    "user_input.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=10, pady=10)\n",
    "\n",
    "send_button = tk.Button(input_frame, text=\"Send\", command=send_query, font=(\"Arial\", 28))  # Larger button font\n",
    "send_button.pack(side=tk.RIGHT, padx=10)\n",
    "\n",
    "# Quiz button\n",
    "quiz_button = tk.Button(root, text=\"Take Our Quiz\", command=personality_quiz, font=(\"Arial\", 28), pady=10)  # Larger font for button\n",
    "quiz_button.pack(pady=20)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6204699,
     "sourceId": 10067374,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6297056,
     "sourceId": 10191750,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "book-recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
